apiVersion: batch/v1
kind: Job
metadata:
  name: ${JOB_NAME}
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      priorityClassName: research-low
      containers:
        - name: worker
          image: lsx-harbor.informatik.uni-wuerzburg.de/steininger-statistical-downscaling/sd-next:1.1.1
          imagePullPolicy: "Always"
          resources:
            limits:
              cpu: "3"
              memory: "20Gi"
              nvidia.com/gpu: "1"
            requests:
              cpu: "3"
              memory: "20Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - mountPath: "/workspace/results"
              name: results
            - mountPath: "/scratch"
              name: scratch
            - mountPath: "/workspace/config.ini"
              name: config
              subPath: "${CONFIG_NAME}"
            - mountPath: "/dev/shm"
              name: shm
          command: ["python", "-u", "run.py", "--num_workers", "3"] # Train + inference
          # command: ["python", "run.py", "--num_workers", "2", "--overfit_on_few_samples"] # Train + inference
          # command: ["tail", "-f", "/dev/null"] # Train + inference
      securityContext:
        runAsUser: 1148
        fsGroup: 1003
      imagePullSecrets:
        - name: lsx-harbor
      volumes:
        - name: results
          cephfs:
            monitors:
              - 132.187.14.16,132.187.14.17,132.187.14.19,132.187.14.20
            user: steininger
            path: "/home/ls6/steininger/results/sd-next"
            secretRef:
              name: ceph-secret
        - name: scratch
          cephfs:
            monitors:
              - 132.187.14.16,132.187.14.17,132.187.14.19,132.187.14.20
            user: steininger
            path: "/scratch"
            secretRef:
              name: ceph-secret
        - name: config
          configMap:
            name: "${CONFIGMAP_NAME}"
        # This allows shared memory allocation of up to the requested amount of memory
        # (without this only 64 MB are available)
        # see: https://www.ibm.com/blogs/research/2018/12/training-cloud-small-files/
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "10Gi"
      restartPolicy: Never
  backoffLimit: 0
  completions: 1
  parallelism: 1
