apiVersion: batch/v1
kind: Job
metadata:
  name: ${JOB_NAME}
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      tolerations:
        - key: "network"
          operator: "Equal"
          value: "slow"
          effect: "NoSchedule"
        # - key: "A100"
        #   operator: "Equal"
        #   value: "true"
        #   effect: "NoSchedule"
      # affinity for same GPU across time tests
      # affinity:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #         - matchExpressions:
      #           - key: nvidia.com/gpu.product
      #             operator: In
      #             values:
      #             - NVIDIA-GeForce-RTX-2080-Ti
      priorityClassName: research-low
      # priorityClassName: research-med # avoid eviction for time tests
      containers:
        - name: worker
          image: lsx-harbor.informatik.uni-wuerzburg.de/steininger-statistical-downscaling/sd-next:1.4.17
          imagePullPolicy: "Always"
          resources:
            limits:
              cpu: "3"
              memory: "25Gi"
              nvidia.com/gpu: "1"
            requests:
              cpu: "3"
              memory: "25Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - mountPath: "/workspace/results"
              name: results
            - mountPath: "/scratch"
              name: scratch
            - mountPath: "/workspace/config.ini"
              name: config
              subPath: "${CONFIG_NAME}"
            - mountPath: "/dev/shm"
              name: shm
          command: ["python", "-u", "run.py", "--num_workers", "3"] # Train + inference
          # command: ["python", "run.py", "--num_workers", "2", "--overfit_on_few_samples"] # Train + inference
          # command: ["tail", "-f", "/dev/null"] # Train + inference
      securityContext:
        runAsUser: 1148
        fsGroup: 1003
      imagePullSecrets:
        - name: lsx-harbor-sd
      volumes:
        - name: results
          cephfs:
            monitors:
              - 132.187.14.16,132.187.14.17,132.187.14.19,132.187.14.20
            user: steininger
            # path: "/home/ls6/steininger/results/sd-next"
            path: "/home/ls6/steininger/results/sd-next/tmp"
            # path: "/home/ls6/steininger/results/sd-next/time_tests"
            # path: "/home/ls6/steininger/results/sd-next/resnet"
            # path: "/home/ls6/steininger/results/sd-next/gggl"
            # path: "/home/ls6/steininger/results/sd-next/ablation"
            # path: "/home/ls6/steininger/results/sd-next/ablation_2"
            # path: "/home/ls6/steininger/results/sd-next/maskedloss"
            secretRef:
              name: ceph-secret
        - name: scratch
          cephfs:
            monitors:
              - 132.187.14.16,132.187.14.17,132.187.14.19,132.187.14.20
            user: steininger
            path: "/scratch"
            secretRef:
              name: ceph-secret
        - name: config
          configMap:
            name: "${CONFIGMAP_NAME}"
        # This allows shared memory allocation of up to the requested amount of memory
        # (without this only 64 MB are available)
        # see: https://www.ibm.com/blogs/research/2018/12/training-cloud-small-files/
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "10Gi"
      restartPolicy: Never
  backoffLimit: 0
  completions: 1
  parallelism: 1
